# ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code (K-Fold í†µí•© ë²„ì „)
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì¼ë°˜ í•™ìŠµê³¼ K-Fold êµì°¨ ê²€ì¦ í•™ìŠµì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
# âœ¨ ìµœì¢… ìˆ˜ì •: ì—”í‹°í‹° ì¹˜í™˜(Entity Swapping) ë°ì´í„° ì¦ê°• ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

# ì‹¤í–‰ ë°©ë²•:
# - ì¼ë°˜ í•™ìŠµ: python kfold_train.py
# - K-Fold í•™ìŠµ: python kfold_train.py --kfold --n_splits 5

# âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •
import pandas as pd
import os
import re
import json
import yaml
import numpy as np
import argparse # ëª¨ë“œ ì „í™˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from glob import glob
from tqdm import tqdm
from pprint import pprint
import torch
from sklearn.model_selection import KFold
from rouge import Rouge
from torch.utils.data import Dataset , DataLoader
from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback
import wandb
from datetime import datetime
import copy # âœ¨ K-Fold ì„¤ì •ì„ ìœ„í•´ ì¶”ê°€
import random # âœ¨ ì—”í‹°í‹° ì¹˜í™˜ì„ ìœ„í•´ ì¶”ê°€

# --- âœ¨ ì—¬ê¸°ì— 'ì—”í‹°í‹° ì¹˜í™˜' ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ì¶”ê°€! âœ¨ ---
def augment_entity_swapping(dialogue, summary):
    """
    ëŒ€í™”ì™€ ìš”ì•½ ìŒì—ì„œ #Person í† í°ë“¤ì„ ì¼ê´€ë˜ê²Œ ë°”ê¿”ì¹˜ê¸°í•˜ì—¬
    ìƒˆë¡œìš´ 'ê°€ì§œ ì—°ìŠµë¬¸ì œ' ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    person_tokens = set(re.findall(r"(#Person\d+#)", dialogue + summary))
    if len(person_tokens) < 2:
        return dialogue, summary
    
    original_tokens = list(person_tokens)
    shuffled_tokens = random.sample(original_tokens, len(original_tokens))
    
    while any(o == s for o, s in zip(original_tokens, shuffled_tokens)):
        random.shuffle(shuffled_tokens)
        
    swap_map = dict(zip(original_tokens, shuffled_tokens))
    
    new_dialogue = dialogue
    new_summary = summary
    for old, new in swap_map.items():
        new_dialogue = re.sub(r'\b' + re.escape(old) + r'\b', new, new_dialogue)
        new_summary = re.sub(r'\b' + re.escape(old) + r'\b', new, new_summary)
        
    return new_dialogue, new_summary
# ----------------------------------------------------------------

# 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶• (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
class Preprocess:
    def __init__(self, bos_token: str, eos_token: str) -> None:
        self.bos_token = bos_token
        self.eos_token = eos_token
    def make_input(self, dataset, is_test = False):
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            decoder_input = dataset['summary'].apply(lambda x : self.bos_token + str(x))
            decoder_output = dataset['summary'].apply(lambda x : str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()

class DatasetForTrain(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len
    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids'); item2.pop('attention_mask')
        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]
        return item
    def __len__(self):
        return self.len

class DatasetForVal(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len
    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids'); item2.pop('attention_mask')
        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]
        return item
    def __len__(self):
        return self.len

def prepare_train_dataset(config, preprocessor, tokenizer, train_data, val_data):
    print('-'*150)
    print(f'train_data:\n {train_data["dialogue"].iloc[0]}')
    print(f'train_label:\n {train_data["summary"].iloc[0]}')
    print('-'*150)
    print(f'val_data:\n {val_data["dialogue"].iloc[0]}')
    print(f'val_label:\n {val_data["summary"].iloc[0]}')
    encoder_input_train , decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)
    encoder_input_val , decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    print('-'*10, 'Load data complete', '-'*10)
    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)
    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs, len(encoder_input_train))
    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)
    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors="pt", padding=True, add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs, len(encoder_input_val))
    print('-'*10, 'Make dataset complete', '-'*10)
    return train_inputs_dataset, val_inputs_dataset

# 2. Trainer ë° ê¸°íƒ€ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
def compute_metrics(config,tokenizer,pred):
    rouge = Rouge()
    predictions = pred.predictions
    labels = pred.label_ids
    predictions[predictions == -100] = tokenizer.pad_token_id
    labels[labels == -100] = tokenizer.pad_token_id
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    results = rouge.get_scores(decoded_preds, labels, avg=True)
    result = {key: value["f"] for key, value in results.items()}
    return result

def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):
    training_args = Seq2SeqTrainingArguments(
        output_dir=config['general']['output_dir'],
        overwrite_output_dir=config['training']['overwrite_output_dir'],
        num_train_epochs=config['training']['num_train_epochs'],
        learning_rate=config['training']['learning_rate'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        warmup_ratio=config['training']['warmup_ratio'],
        weight_decay=config['training']['weight_decay'],
        lr_scheduler_type=config['training']['lr_scheduler_type'],
        optim =config['training']['optim'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        evaluation_strategy=config['training']['evaluation_strategy'],
        save_strategy =config['training']['save_strategy'],
        save_total_limit=config['training']['save_total_limit'],
        fp16=config['training']['fp16'],
        load_best_model_at_end=config['training']['load_best_model_at_end'],
        seed=config['training']['seed'],
        logging_dir=config['training']['logging_dir'],
        logging_strategy=config['training']['logging_strategy'],
        predict_with_generate=config['training']['predict_with_generate'],
        generation_max_length=config['training']['generation_max_length'],
        do_train=config['training']['do_train'],
        do_eval=config['training']['do_eval'],
        report_to=config['training']['report_to']
    )
    if config['training']['report_to'] == 'wandb':
        wandb.init(
            entity=config['wandb']['entity'],
            project=config['wandb']['project'],
            name=config['wandb']['name'],
            reinit=True
        )
        os.environ["WANDB_LOG_MODEL"]="true"
        os.environ["WANDB_WATCH"]="false"
    
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=config['training']['early_stopping_patience'],
        early_stopping_threshold=config['training']['early_stopping_threshold']
    )
    trainer = Seq2SeqTrainer(
        model=generate_model,
        args=training_args,
        train_dataset=train_inputs_dataset,
        eval_dataset=val_inputs_dataset,
        compute_metrics = lambda pred: compute_metrics(config, tokenizer, pred),
        callbacks = [MyCallback]
    )
    return trainer

def load_tokenizer_and_model_for_train(config, device):
    model_name = config['general']['model_name']
    bart_config = BartConfig.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    generate_model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)
    if 'special_tokens' in config['tokenizer']:
        special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}
        tokenizer.add_special_tokens(special_tokens_dict)
        generate_model.resize_token_embeddings(len(tokenizer))
    generate_model.to(device)
    return generate_model, tokenizer

# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
def main(config):
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print('-'*10, f'Device: {device}', '-'*10)
    data_path = config['general']['data_path']
    train_df = pd.read_csv(os.path.join(data_path,'train.csv'))
    val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))
    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])
    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, tokenizer, train_df, val_df)
    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)
    trainer.train()
    if config['training']['report_to'] == 'wandb':
        wandb.finish()

def kfold_main(config, n_splits):
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    data_path = config['general']['data_path']
    train_file_path = os.path.join(data_path, 'train.csv')
    train_df = pd.read_csv(train_file_path)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=config['training']['seed'])
    all_eval_scores = []
    base_output_dir = config['general'].get('output_dir', './output/kfold_experiment')
    if not os.path.exists(base_output_dir):
        os.makedirs(base_output_dir)
        
    for fold, (train_index, val_index) in enumerate(kf.split(train_df)):
        print(f"\n{'='*25} Fold {fold+1}/{n_splits} Training Start {'='*25}")
        
        generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)
        train_fold_df = train_df.iloc[train_index].reset_index(drop=True)
        val_fold_df = train_df.iloc[val_index].reset_index(drop=True)

        # --- âœ¨ ì—¬ê¸°ì— ë°ì´í„° ì¦ê°•(íŠ¹ë³„ ê³¼ì™¸) ì½”ë“œ ì¶”ê°€! âœ¨ ---
        print(f"Original fold training data size: {len(train_fold_df)}")
        augmented_data = []
        # ì›ë³¸ ë°ì´í„° 1ê°œë‹¹ 2ê°œì˜ 'ê°€ì§œ ì—°ìŠµë¬¸ì œ'ë¥¼ ë§Œë“­ë‹ˆë‹¤ (í•„ìš”ì‹œ íšŸìˆ˜ ì¡°ì ˆ)
        for _ in range(2): 
            for _, row in train_fold_df.iterrows():
                new_dialogue, new_summary = augment_entity_swapping(row['dialogue'], row['summary'])
                augmented_data.append({'fname': row['fname'], 'dialogue': new_dialogue, 'summary': new_summary})
        
        augmented_df = pd.DataFrame(augmented_data)
        # ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„°ë¥¼ í•©ì¹˜ê³  ìˆœì„œë¥¼ ì„ìŠµë‹ˆë‹¤.
        train_fold_df = pd.concat([train_fold_df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)
        print(f"Fold training data size after augmentation: {len(train_fold_df)}")
        # ----------------------------------------------------

        config_fold = copy.deepcopy(config)
        fold_output_dir = os.path.join(base_output_dir, f"fold_{fold+1}")
        config_fold['general']['output_dir'] = fold_output_dir
        config_fold['training']['logging_dir'] = os.path.join(fold_output_dir, 'logs')
        if config_fold.get('training', {}).get('report_to') == 'wandb':
            original_wandb_name = config_fold.get('wandb', {}).get('name', 'kfold_run')
            config_fold['wandb']['name'] = f"{original_wandb_name}_fold_{fold+1}"
            
        preprocessor = Preprocess(config_fold['tokenizer']['bos_token'], config_fold['tokenizer']['eos_token'])
        train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config_fold, preprocessor, tokenizer, train_fold_df, val_fold_df)
        trainer = load_trainer_for_train(config_fold, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)
        trainer.train()
        eval_results = trainer.evaluate()
        print(f"\nFold {fold+1} Evaluation Results: {eval_results}")
        all_eval_scores.append(eval_results['eval_rouge-1'])
        if config_fold.get('training', {}).get('report_to') == 'wandb':
            wandb.finish()
            
    print(f"\n{'='*25} K-Fold Cross-Validation Final Results {'='*25}")
    print(f"All Fold Scores (rouge-1): {all_eval_scores}")
    print(f"Average Score (rouge-1): {np.mean(all_eval_scores):.4f}")
    print(f"Standard Deviation: {np.std(all_eval_scores):.4f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument('--kfold', action='store_true', help="K-Fold êµì°¨ ê²€ì¦ ëª¨ë“œë¡œ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    parser.add_argument('--n_splits', type=int, default=5, help="K-Foldì— ì‚¬ìš©í•  í´ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)")
    args = parser.parse_args()
    config_path = "./config.yaml"
    with open(config_path, "r") as file:
        loaded_config = yaml.safe_load(file)
    if args.kfold:
        print("K-Fold êµì°¨ ê²€ì¦ ëª¨ë“œë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        kfold_main(loaded_config, n_splits=args.n_splits)
    else:
        print("ì¼ë°˜ í•™ìŠµ ëª¨ë“œë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        main(loaded_config)
